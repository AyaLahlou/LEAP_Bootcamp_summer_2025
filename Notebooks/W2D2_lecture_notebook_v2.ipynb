{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/thunderhoser/cira_ml_short_course/blob/master/lecture05_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osxWFk0-PBMN"
   },
   "source": [
    "# Convolutional neural networks (CNN)\n",
    "\n",
    "This notebook was adapted from  Lagerquist, Ryan, and David John Gagne II, 2020: \"Lecture 5: Convolutional neural networks\". \n",
    "\n",
    " <font color='red'>Note: Make sure to use a GPU (graphical processing unit) to run this notebook!</font>\n",
    "The CNNs in this notebook run $\\sim$100 times faster with a GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxE0Nv5qPNGI"
   },
   "source": [
    "# <font color='red'>Clone the Git repository (required)</font>\n",
    "\n",
    "- **Please note**: when a section title is in <font color='red'>red</font>, that means the code cell below is required.\n",
    "- In other words, if you don't run the code cell below, subsequent code cells might not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Akd8DG2ZPG4A",
    "outputId": "6fa2cb5c-e6d3-47f9-b9d2-7d8938837582"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.isdir('course_repository'):\n",
    "    shutil.rmtree('course_repository')\n",
    "\n",
    "!git clone https://github.com/thunderhoser/cira_ml_short_course course_repository\n",
    "!cd course_repository; python setup.py install\n",
    "\n",
    "!pip uninstall -y netCDF4\n",
    "!pip install netCDF4\n",
    "\n",
    "!pip uninstall -y cftime\n",
    "!pip install cftime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-iV7mXNPZt7"
   },
   "source": [
    "# <font color='red'>Import packages (required)</font>\n",
    "\n",
    "The next two cells import all packages used in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YzgwFW-HPSol"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/content/data/')\n",
    "sys.path.append('/content/course_repository/')\n",
    "sys.path.append('/content/course_repository/cira_ml_short_course/')\n",
    "sys.path.append('/content/course_repository/cira_ml_short_course/utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RGkxBv8NPbDT",
    "outputId": "573633fc-47ef-448a-e282-b38af74f5a9c"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import copy\n",
    "import random\n",
    "import os.path\n",
    "import warnings\n",
    "import numpy\n",
    "import keras\n",
    "from matplotlib import pyplot\n",
    "import tensorflow.compat.v1 as tf\n",
    "from cira_ml_short_course.utils import utils, image_utils, \\\n",
    "    image_normalization, image_thresholding, cnn, upconvnet, saliency, \\\n",
    "    class_activation, novelty_detection\n",
    "from cira_ml_short_course.utils import backwards_optimization as backwards_opt\n",
    "from cira_ml_short_course.plotting import image_plotting, permutation_plotting\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEPARATOR_STRING = '\\n\\n' + '*' * 50 + '\\n\\n'\n",
    "MINOR_SEPARATOR_STRING = '\\n\\n' + '-' * 50 + '\\n\\n'\n",
    "\n",
    "DATA_DIRECTORY_NAME = '/content/data/track_data_ncar_ams_3km_nc_small'\n",
    "\n",
    "BEST_HIT_MATRIX_KEY = 'best_hits_predictor_matrix'\n",
    "WORST_FALSE_ALARM_MATRIX_KEY = 'worst_false_alarms_predictor_matrix'\n",
    "WORST_MISS_MATRIX_KEY = 'worst_misses_predictor_matrix'\n",
    "BEST_CORRECT_NULLS_MATRIX_KEY = 'best_correct_nulls_predictor_matrix'\n",
    "PREDICTOR_NAMES_KEY = 'predictor_names'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RMigT2OR1oI"
   },
   "source": [
    "# <font color='red'>Prevent auto-scrolling (required)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "4G4qCf_1PsP6",
    "outputId": "fa1e1045-21ea-4f51-9576-b59fcc15b057"
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQ061WGkR3pL"
   },
   "source": [
    "# <font color='red'>Download input data (required)</font>\n",
    "\n",
    "The next cell downloads all input data used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9SPruH3gR27c",
    "outputId": "c5161dbb-8feb-4c2c-9b0a-cd7fdab50409"
   },
   "outputs": [],
   "source": [
    "!python /content/course_repository/download_image_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQeZ2o96R_Hm"
   },
   "source": [
    "# <font color='red'>Read input data (required)</font>\n",
    "\n",
    "The next cell reads all input data for this notebook into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B20uf1YKR72S",
    "outputId": "ef6d70d8-1494-45a4-ef2f-6f7e05d634b1"
   },
   "outputs": [],
   "source": [
    "training_file_names = image_utils.find_many_files(\n",
    "    first_date_string='20100101', last_date_string='20141224',\n",
    "    directory_name=DATA_DIRECTORY_NAME\n",
    ")\n",
    "\n",
    "validation_file_names = image_utils.find_many_files(\n",
    "    first_date_string='20150101', last_date_string='20151231',\n",
    "    directory_name=DATA_DIRECTORY_NAME\n",
    ")\n",
    "\n",
    "testing_file_names = image_utils.find_many_files(\n",
    "    first_date_string='20160101', last_date_string='20171231',\n",
    "    directory_name=DATA_DIRECTORY_NAME\n",
    ")\n",
    "\n",
    "training_image_dict = image_utils.read_many_files(training_file_names)\n",
    "print('\\n')\n",
    "\n",
    "validation_image_dict = image_utils.read_many_files(validation_file_names)\n",
    "print('\\n')\n",
    "\n",
    "testing_image_dict = image_utils.read_many_files(testing_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtZUbWNn0oik"
   },
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HempzkIEkiXV"
   },
   "source": [
    "## Plot random example with wind barbs\n",
    "\n",
    " - The next cell plots a random example with wind barbs.\n",
    " - One example = one storm object = one storm cell at one time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "zqaah2ub0saJ",
    "outputId": "efdada71-a9a4-4cb1-9e2c-0f5cd24792a8"
   },
   "outputs": [],
   "source": [
    "predictor_matrix = (\n",
    "    validation_image_dict[image_utils.PREDICTOR_MATRIX_KEY]\n",
    ")\n",
    "predictor_names = validation_image_dict[image_utils.PREDICTOR_NAMES_KEY]\n",
    "\n",
    "num_examples = predictor_matrix.shape[0]\n",
    "random_index = random.randint(0, num_examples - 1)\n",
    "predictor_matrix = predictor_matrix[random_index, ...]\n",
    "\n",
    "temperature_matrix_kelvins = predictor_matrix[\n",
    "    ..., predictor_names.index(image_utils.TEMPERATURE_NAME)\n",
    "]\n",
    "min_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 1)\n",
    "max_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 99)\n",
    "\n",
    "image_plotting.plot_many_predictors_with_barbs(\n",
    "    predictor_matrix=predictor_matrix, predictor_names=predictor_names,\n",
    "    min_colour_temp_kelvins=min_temp_kelvins,\n",
    "    max_colour_temp_kelvins=max_temp_kelvins\n",
    ")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPSPHiO_lsAq"
   },
   "source": [
    "## Plot strong example with wind barbs\n",
    "\n",
    "The next cell plots the strongest example in the validation data (that with the greatest max future vorticity), using wind barbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "id": "yfcG8pr7luw1",
    "outputId": "8d2ee354-7378-4d64-c3df-137a9ceca243"
   },
   "outputs": [],
   "source": [
    "target_matrix_s01 = validation_image_dict[image_utils.TARGET_MATRIX_KEY]\n",
    "example_index = numpy.unravel_index(\n",
    "    numpy.argmax(target_matrix_s01), target_matrix_s01.shape\n",
    ")[0]\n",
    "\n",
    "predictor_matrix = validation_image_dict[image_utils.PREDICTOR_MATRIX_KEY][\n",
    "    example_index, ...\n",
    "]\n",
    "predictor_names = validation_image_dict[image_utils.PREDICTOR_NAMES_KEY]\n",
    "\n",
    "temperature_matrix_kelvins = predictor_matrix[\n",
    "    ..., predictor_names.index(image_utils.TEMPERATURE_NAME)\n",
    "]\n",
    "min_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 1)\n",
    "max_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 99)\n",
    "\n",
    "image_plotting.plot_many_predictors_with_barbs(\n",
    "    predictor_matrix=predictor_matrix, predictor_names=predictor_names,\n",
    "    min_colour_temp_kelvins=min_temp_kelvins,\n",
    "    max_colour_temp_kelvins=max_temp_kelvins\n",
    ")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Gmz02iGnR55"
   },
   "source": [
    "## Plot random example without wind barbs\n",
    "\n",
    " - The next cell plots a random example without wind barbs.\n",
    " - In this case, the wind field is plotted as two scalar fields ($u$-wind and $v$-wind).\n",
    " - This plotting format will be used when interpretation quantities (represented by line contours) are overlain on the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "id": "eJ52M3v4nkNv",
    "outputId": "d2bc1377-6e42-4929-a731-1954a09773cd"
   },
   "outputs": [],
   "source": [
    "predictor_matrix = (\n",
    "    validation_image_dict[image_utils.PREDICTOR_MATRIX_KEY]\n",
    ")\n",
    "predictor_names = validation_image_dict[image_utils.PREDICTOR_NAMES_KEY]\n",
    "\n",
    "num_examples = predictor_matrix.shape[0]\n",
    "random_index = random.randint(0, num_examples - 1)\n",
    "predictor_matrix = predictor_matrix[random_index, ...]\n",
    "\n",
    "temperature_matrix_kelvins = predictor_matrix[\n",
    "    ..., predictor_names.index(image_utils.TEMPERATURE_NAME)\n",
    "]\n",
    "min_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 1)\n",
    "max_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 99)\n",
    "\n",
    "wind_speed_matrix_m_s01 = numpy.sqrt(\n",
    "    predictor_matrix[..., predictor_names.index(image_utils.U_WIND_NAME)] ** 2 +\n",
    "    predictor_matrix[..., predictor_names.index(image_utils.V_WIND_NAME)] ** 2\n",
    ")\n",
    "max_speed_m_s01 = numpy.percentile(\n",
    "    numpy.absolute(wind_speed_matrix_m_s01), 99\n",
    ")\n",
    "\n",
    "image_plotting.plot_many_predictors_sans_barbs(\n",
    "    predictor_matrix=predictor_matrix, predictor_names=predictor_names,\n",
    "    min_colour_temp_kelvins=min_temp_kelvins,\n",
    "    max_colour_temp_kelvins=max_temp_kelvins,\n",
    "    max_colour_wind_speed_m_s01=max_speed_m_s01\n",
    ")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7Sx3jfbn1JJ"
   },
   "source": [
    "## Plot strong example without wind barbs\n",
    "\n",
    "The next cell plots the strongest example in the validation data (that with the greatest max future vorticity), using scalar fields instead of wind barbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "id": "rPuN_F-9n3Uz",
    "outputId": "69e628d3-95a3-4f45-92ad-c3917d478549"
   },
   "outputs": [],
   "source": [
    "target_matrix_s01 = validation_image_dict[image_utils.TARGET_MATRIX_KEY]\n",
    "example_index = numpy.unravel_index(\n",
    "    numpy.argmax(target_matrix_s01), target_matrix_s01.shape\n",
    ")[0]\n",
    "\n",
    "predictor_matrix = validation_image_dict[image_utils.PREDICTOR_MATRIX_KEY][\n",
    "    example_index, ...\n",
    "]\n",
    "predictor_names = validation_image_dict[image_utils.PREDICTOR_NAMES_KEY]\n",
    "\n",
    "temperature_matrix_kelvins = predictor_matrix[\n",
    "    ..., predictor_names.index(image_utils.TEMPERATURE_NAME)\n",
    "]\n",
    "min_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 1)\n",
    "max_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 99)\n",
    "\n",
    "wind_speed_matrix_m_s01 = numpy.sqrt(\n",
    "    predictor_matrix[..., predictor_names.index(image_utils.U_WIND_NAME)] ** 2 +\n",
    "    predictor_matrix[..., predictor_names.index(image_utils.V_WIND_NAME)] ** 2\n",
    ")\n",
    "max_speed_m_s01 = numpy.percentile(\n",
    "    numpy.absolute(wind_speed_matrix_m_s01), 99\n",
    ")\n",
    "\n",
    "image_plotting.plot_many_predictors_sans_barbs(\n",
    "    predictor_matrix=predictor_matrix, predictor_names=predictor_names,\n",
    "    min_colour_temp_kelvins=min_temp_kelvins,\n",
    "    max_colour_temp_kelvins=max_temp_kelvins,\n",
    "    max_colour_wind_speed_m_s01=max_speed_m_s01\n",
    ")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKlyKneFS-WK"
   },
   "source": [
    "# <font color='red'>Find normalization parameters (required)</font>\n",
    "\n",
    "- The next cell finds normalization parameters for each predictor variable.\n",
    "- Recall from Lecture 1 that normalization parameters are based on the training data only.\n",
    "- For $z$-score normalization, the parameters are mean and standard deviation.\n",
    "<br><br>\n",
    "\n",
    "- we still compute one mean and standard deviation for each predictor, rather than one for each predictor and grid cell.\n",
    "- Thus, the mean and standard deviation for each predictor are based on data from all training examples and all 1024 grid cells.\n",
    "<br><br>\n",
    "\n",
    "- We will not normalize the data right away.\n",
    "- We will plot the data frequently throughout this notebook, and we want to plot variables in physical space rather than normalized space.\n",
    "- However, we will store the normalization parameters and use them when feeding data into a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCdl_BImSUcz"
   },
   "outputs": [],
   "source": [
    "normalization_dict = image_normalization.get_normalization_params(\n",
    "    image_dict=training_image_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOoBoflLT2U6"
   },
   "source": [
    "## Sanity check\n",
    "\n",
    "- The next cell normalizes, and then denormalizes, a small amount of testing data.\n",
    "- Normalized values should be small positive or negative numbers (mostly from $-3\\ldots+3$).\n",
    "- Denormalized values must equal original (physical) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "_4Kwib9VTvq0",
    "outputId": "c6ea03fd-4276-4158-e0c1-3229ae1103ab"
   },
   "outputs": [],
   "source": [
    "predictor_names = testing_image_dict[image_utils.PREDICTOR_NAMES_KEY]\n",
    "original_values = (\n",
    "    testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY][0, :5, :5, 0]\n",
    ")\n",
    "\n",
    "print('\\nOriginal values of {0:s} for first storm object:\\n{1:s}'.format(\n",
    "    predictor_names[0], str(original_values)\n",
    "))\n",
    "\n",
    "testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY], _ = (\n",
    "    image_normalization.normalize_data(\n",
    "        predictor_matrix=testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY],\n",
    "        predictor_names=predictor_names,\n",
    "        normalization_dict=normalization_dict\n",
    "    )\n",
    ")\n",
    "\n",
    "normalized_values = (\n",
    "    testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY][0, :5, :5, 0]\n",
    ")\n",
    "print((\n",
    "    '\\nNormalized values of {0:s} for first storm object:\\n{1:s}'\n",
    ").format(\n",
    "    predictor_names[0], str(normalized_values)\n",
    "))\n",
    "\n",
    "testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY] = (\n",
    "    image_normalization.denormalize_data(\n",
    "        predictor_matrix=testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY],\n",
    "        predictor_names=predictor_names,\n",
    "        normalization_dict=normalization_dict\n",
    "    )\n",
    ")\n",
    "\n",
    "denormalized_values = (\n",
    "    testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY][0, :5, :5, 0]\n",
    ")\n",
    "print((\n",
    "    '\\nDenormalized values of {0:s} for first storm object:\\n{1:s}'\n",
    ").format(\n",
    "    predictor_names[0], str(denormalized_values)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjT0WD8CWE5M"
   },
   "source": [
    "# <font color='red'>Binarization (required)</font>\n",
    "\n",
    "- The next cell binarizes the target variable (max future vorticity in s$^{-1}$).\n",
    "- **However, CNN can also perform regression.**\n",
    "<br><br>\n",
    "\n",
    "- we will maximize over the 1024 grid cells to get a scalar target variable again.\n",
    "<br><br>\n",
    "\n",
    "- we will not binarize the target variable right away.\n",
    "- However, we will store the binarization threshold and use it when feeding data into a CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHjW4_J3UtZh",
    "outputId": "d92605ef-4725-4e53-b49a-50a70702c15c"
   },
   "outputs": [],
   "source": [
    "binarization_threshold = image_thresholding.get_binarization_threshold(\n",
    "    image_dict=training_image_dict, percentile_level=90.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTLrcsjSXDjP"
   },
   "source": [
    "## Sanity check\n",
    "\n",
    "- The next cell binarizes the target variable for a small amount of testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "DXNUVerdW9sJ",
    "outputId": "cc0f440a-a833-41ed-fc3e-1e7fbe62923d"
   },
   "outputs": [],
   "source": [
    "target_matrix_s01 = copy.deepcopy(\n",
    "    testing_image_dict[image_utils.TARGET_MATRIX_KEY]\n",
    ")\n",
    "spatial_maxima_s01 = numpy.max(target_matrix_s01, axis=(1, 2))\n",
    "\n",
    "print((\n",
    "    '\\nSpatial maxima of {0:s} for the first few storm objects:\\n{1:s}'\n",
    ").format(\n",
    "    testing_image_dict[image_utils.TARGET_NAME_KEY],\n",
    "    str(spatial_maxima_s01[:20])\n",
    "))\n",
    "\n",
    "target_classes = image_thresholding.binarize_target_images(\n",
    "    target_matrix=testing_image_dict[image_utils.TARGET_MATRIX_KEY],\n",
    "    binarization_threshold=binarization_threshold\n",
    ")\n",
    "\n",
    "print((\n",
    "    '\\nBinarized target values (classes) for the first few storm objects:'\n",
    "    '\\n{0:s}'\n",
    ").format(\n",
    "    str(target_classes[:20])\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_KNLjvAp5Z5"
   },
   "source": [
    "# Basic example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSB-px22YbUW"
   },
   "source": [
    "## Architecture\n",
    "\n",
    "The next cell creates a CNN with the following hyperparameters:\n",
    "\n",
    " - 4 convolutional blocks\n",
    " - 2 convolutional layers per block\n",
    " - Dropout rate for conv layers and output layer = 0\n",
    " - Dropout rate for non-terminal dense layers = 0.5\n",
    " - Activation function for conv layers and non-terminal dense layers = leaky ReLU with slope of 0.2\n",
    " - Activation function for output layer = sigmoid (binary classification)\n",
    " - No L$_1$ regularization\n",
    " - L$_2$ regularization (strength of 0.001) for all convolutional layers\n",
    "<br><br>\n",
    "\n",
    "The other hyperparameters, which are all architectural (*i.e.*, layer types and sizes), are shown in the table printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-SjR26h7XunZ",
    "outputId": "18b5cad9-ebb5-49e7-8b7d-2aacb07dde11"
   },
   "outputs": [],
   "source": [
    "default_model_object = cnn.setup_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IdzVZbL0WVm9",
    "outputId": "0a6eab5c-63b8-4a15-a2c0-d08d321f9425"
   },
   "outputs": [],
   "source": [
    "# DEFAULT_INPUT_DIMENSIONS = numpy.array([32, 32, 4], dtype=int)\n",
    "# DEFAULT_CONV_BLOCK_LAYER_COUNTS = numpy.array([2, 2, 2, 2], dtype=int)\n",
    "# DEFAULT_CONV_CHANNEL_COUNTS = numpy.array(\n",
    "#     [32, 32, 64, 64, 128, 128, 256, 256], dtype=int\n",
    "# )\n",
    "# DEFAULT_CONV_DROPOUT_RATES = numpy.full(8, 0.)\n",
    "# DEFAULT_CONV_FILTER_SIZES = numpy.full(8, 3, dtype=int)\n",
    "# DEFAULT_DENSE_NEURON_COUNTS = numpy.array([776, 147, 28, 5, 1], dtype=int)\n",
    "# DEFAULT_DENSE_DROPOUT_RATES = numpy.array([0.5, 0.5, 0.5, 0.5, 0])\n",
    "# DEFAULT_INNER_ACTIV_FUNCTION_NAME = copy.deepcopy(utils.RELU_FUNCTION_NAME)\n",
    "# DEFAULT_INNER_ACTIV_FUNCTION_ALPHA = 0.2\n",
    "# DEFAULT_OUTPUT_ACTIV_FUNCTION_NAME = copy.deepcopy(utils.SIGMOID_FUNCTION_NAME)\n",
    "# DEFAULT_OUTPUT_ACTIV_FUNCTION_ALPHA = 0.\n",
    "# DEFAULT_L1_WEIGHT = 0.\n",
    "# DEFAULT_L2_WEIGHT = 0.001\n",
    "\n",
    "cnn.setup_cnn(\n",
    "    input_dimensions=cnn.DEFAULT_INPUT_DIMENSIONS,\n",
    "    conv_block_layer_counts=numpy.array([1, 1, 1, 1], dtype=int),\n",
    "    conv_layer_channel_counts=numpy.array([32, 64, 128, 256], dtype=int),\n",
    "    conv_layer_dropout_rates=numpy.full(4, 0.),\n",
    "    conv_layer_filter_sizes=numpy.full(4, 3, dtype=int),\n",
    "    dense_layer_neuron_counts=cnn.DEFAULT_DENSE_NEURON_COUNTS,\n",
    "    dense_layer_dropout_rates=cnn.DEFAULT_DENSE_DROPOUT_RATES,\n",
    "    inner_activ_function_name=cnn.DEFAULT_INNER_ACTIV_FUNCTION_NAME,\n",
    "    inner_activ_function_alpha=cnn.DEFAULT_INNER_ACTIV_FUNCTION_ALPHA,\n",
    "    output_activ_function_name=cnn.DEFAULT_OUTPUT_ACTIV_FUNCTION_NAME,\n",
    "    output_activ_function_alpha=cnn.DEFAULT_OUTPUT_ACTIV_FUNCTION_ALPHA,\n",
    "    l1_weight=cnn.DEFAULT_L1_WEIGHT, l2_weight=cnn.DEFAULT_L2_WEIGHT,\n",
    "    use_batch_normalization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJAnXoYUZ0pL"
   },
   "source": [
    "## Training\n",
    "\n",
    "The next cell trains the CNN we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mXYqnqqGYr1k",
    "outputId": "b08a9c0f-eaa6-471c-e81d-216eeeda0312"
   },
   "outputs": [],
   "source": [
    "cnn.train_model_sans_generator(\n",
    "    model_object=default_model_object,\n",
    "    training_file_names=training_file_names,\n",
    "    validation_file_names=validation_file_names,\n",
    "    num_examples_per_batch=1024,\n",
    "    normalization_dict=normalization_dict,\n",
    "    binarization_threshold=binarization_threshold,\n",
    "    num_epochs=100,\n",
    "    output_dir_name='/content/models/default_cnn'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhYRKUcJaK9i"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "The next cell evaluates the CNN we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "mjh4qJ8naHTA",
    "outputId": "6f273dc1-f7f1-488c-a387-5ae7e06cc525"
   },
   "outputs": [],
   "source": [
    "predictor_names = training_image_dict[image_utils.PREDICTOR_NAMES_KEY]\n",
    "\n",
    "training_norm_predictor_matrix, _ = image_normalization.normalize_data(\n",
    "    predictor_matrix=copy.deepcopy(\n",
    "        training_image_dict[image_utils.PREDICTOR_MATRIX_KEY]\n",
    "    ),\n",
    "    predictor_names=predictor_names, normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "training_classes = image_thresholding.binarize_target_images(\n",
    "    target_matrix=training_image_dict[image_utils.TARGET_MATRIX_KEY],\n",
    "    binarization_threshold=binarization_threshold\n",
    ")\n",
    "\n",
    "training_probs = cnn.apply_model(\n",
    "    model_object=default_model_object,\n",
    "    predictor_matrix=training_norm_predictor_matrix\n",
    ")\n",
    "\n",
    "_ = utils.eval_binary_classifn(\n",
    "    observed_labels=training_classes,\n",
    "    forecast_probabilities=training_probs,\n",
    "    training_event_frequency=numpy.mean(training_classes),\n",
    "    dataset_name='training'\n",
    ")\n",
    "\n",
    "validation_norm_predictor_matrix, _ = image_normalization.normalize_data(\n",
    "    predictor_matrix=copy.deepcopy(\n",
    "        validation_image_dict[image_utils.PREDICTOR_MATRIX_KEY]\n",
    "    ),\n",
    "    predictor_names=predictor_names, normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "validation_classes = image_thresholding.binarize_target_images(\n",
    "    target_matrix=validation_image_dict[image_utils.TARGET_MATRIX_KEY],\n",
    "    binarization_threshold=binarization_threshold\n",
    ")\n",
    "\n",
    "validation_probs = cnn.apply_model(\n",
    "    model_object=default_model_object,\n",
    "    predictor_matrix=validation_norm_predictor_matrix\n",
    ")\n",
    "\n",
    "_ = utils.eval_binary_classifn(\n",
    "    observed_labels=validation_classes,\n",
    "    forecast_probabilities=validation_probs,\n",
    "    training_event_frequency=numpy.mean(training_classes),\n",
    "    dataset_name='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XDGIK8Eld6Qu"
   },
   "source": [
    "# <font color='red'>Read pre-trained CNN (required)</font>\n",
    "\n",
    "The next cell reads a pre-trained CNN, to which interpretation methods will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abqArpx5d-mV",
    "outputId": "9cc8b382-eaab-404a-a29d-b2cf996cd655"
   },
   "outputs": [],
   "source": [
    "pretrained_model_file_name = (\n",
    "    '/content/course_repository/pretrained_cnn/model.h5'\n",
    ")\n",
    "\n",
    "pretrained_model_object = utils.read_dense_net(pretrained_model_file_name)\n",
    "pretrained_model_object.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSWjdVqFqZD2"
   },
   "source": [
    "# Interpretation method 1: Permutation importance test (PIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ND9z6_xLeH1U"
   },
   "source": [
    "## Theory\n",
    " - **The PIT measures the importance of each predictor variable, averaged over all examples in a dataset.**\n",
    " - We apply the PIT (and all other interpretation methods) to the testing set.\n",
    "<br><br>\n",
    "\n",
    " - **The \"importance\" of predictor $x_j$ is determined by how much model performance declines when $x_j$ is permuted.**\n",
    " - For scalar predictors, this means randomly shuffling values of $x_j$ over the examples.\n",
    " - For spatial predictors, this means randomly shuffling entire spatial maps of $x_j$ over the examples.\n",
    " - In other words, **spatial maps are kept intact, but the order of spatial maps is permuted.**\n",
    "<br><br>\n",
    "\n",
    " - **There are four versions of the PIT:**\n",
    "   - Single-pass forward test\n",
    "   - Multi-pass forward test\n",
    "   - Single-pass backwards test\n",
    "   - Multi-pass backwards test\n",
    " - The four versions handle correlated predictors in different ways.\n",
    " - The more correlation (interdependence) there is, the more results among the four versions of the test differ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT8W2Bmun1mF"
   },
   "source": [
    "## Run forward versions of test\n",
    "\n",
    " - The next cell runs both forward versions (single-pass and multi-pass) of the permutation importance test.\n",
    " - **The loss function is negative AUC (area under ROC curve).**\n",
    " - At each step, the code computes a 95% confidence interval for the loss, using 1000 bootstrap replicates.\n",
    " - **In each figure, the most (least) important predictor is at the top (bottom)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4XP0dZaimQS8",
    "outputId": "23081766-61e4-44c0-9b50-250ff5580d75"
   },
   "outputs": [],
   "source": [
    "predictor_names = testing_image_dict[image_utils.PREDICTOR_NAMES_KEY]\n",
    "\n",
    "testing_norm_predictor_matrix, _ = image_normalization.normalize_data(\n",
    "    predictor_matrix=copy.deepcopy(\n",
    "        testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY]\n",
    "    ),\n",
    "    predictor_names=predictor_names, normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "testing_classes = image_thresholding.binarize_target_images(\n",
    "    target_matrix=testing_image_dict[image_utils.TARGET_MATRIX_KEY],\n",
    "    binarization_threshold=binarization_threshold\n",
    ")\n",
    "\n",
    "forward_result_dict = utils.run_forward_test(\n",
    "    predictor_matrix=testing_norm_predictor_matrix,\n",
    "    predictor_names=predictor_names,\n",
    "    target_classes=testing_classes,\n",
    "    model_object=pretrained_model_object\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULxJPVNNqGsx"
   },
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AxcX6tdgqHDF",
    "outputId": "74e91313-a556-4574-a053-1cdfa4523b83"
   },
   "outputs": [],
   "source": [
    "axes_object = permutation_plotting.plot_single_pass_test(\n",
    "    result_dict=forward_result_dict\n",
    ")\n",
    "axes_object.set_title('Single-pass forward test')\n",
    "axes_object.set_xlabel('Testing AUC')\n",
    "pyplot.show()\n",
    "print('\\n\\n')\n",
    "\n",
    "axes_object = permutation_plotting.plot_multipass_test(\n",
    "    result_dict=forward_result_dict\n",
    ")\n",
    "axes_object.set_title('Multi-pass forward test')\n",
    "axes_object.set_xlabel('Testing AUC')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_jNV75cs8sf"
   },
   "source": [
    "## Run backwards versions of test\n",
    "\n",
    " - The next cell runs both backwards versions (single-pass and multi-pass) of the permutation importance test.\n",
    " - **In each figure, the most (least) important predictor is at the top (bottom)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 986
    },
    "id": "TYSUsUhiolEv",
    "outputId": "b1c1212c-4cb7-4cb0-db0f-bd46319a720d"
   },
   "outputs": [],
   "source": [
    "predictor_names = testing_image_dict[image_utils.PREDICTOR_NAMES_KEY]\n",
    "\n",
    "testing_norm_predictor_matrix, _ = image_normalization.normalize_data(\n",
    "    predictor_matrix=copy.deepcopy(\n",
    "        testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY]\n",
    "    ),\n",
    "    predictor_names=predictor_names, normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "testing_classes = image_thresholding.binarize_target_images(\n",
    "    target_matrix=testing_image_dict[image_utils.TARGET_MATRIX_KEY],\n",
    "    binarization_threshold=binarization_threshold\n",
    ")\n",
    "\n",
    "backwards_result_dict = utils.run_backwards_test(\n",
    "    predictor_matrix=testing_norm_predictor_matrix,\n",
    "    predictor_names=predictor_names,\n",
    "    target_classes=testing_classes,\n",
    "    model_object=pretrained_model_object\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFLlqm3BtEtf"
   },
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gHqp4hGGtD8g",
    "outputId": "a42a7703-b654-400f-fe5e-8a1468c752a9"
   },
   "outputs": [],
   "source": [
    "axes_object = permutation_plotting.plot_single_pass_test(\n",
    "    result_dict=backwards_result_dict, num_predictors_to_plot=20\n",
    ")\n",
    "axes_object.set_title('Single-pass backwards test')\n",
    "axes_object.set_xlabel('Testing AUC')\n",
    "pyplot.show()\n",
    "print('\\n\\n')\n",
    "\n",
    "axes_object = permutation_plotting.plot_multipass_test(\n",
    "    result_dict=backwards_result_dict, num_predictors_to_plot=20\n",
    ")\n",
    "axes_object.set_title('Multi-pass backwards test')\n",
    "axes_object.set_xlabel('Testing AUC')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lt6F6acarFaE"
   },
   "source": [
    "# Extreme cases for pre-trained CNN\n",
    "\n",
    "There are four types of extreme cases, defined below.  A positive (negative) example is a storm that does (not) develop strong rotation in the future.\n",
    "\n",
    " - **Best hits**: the 100 positive examples with highest forecast probability\n",
    " - **Worst false alarms**: the 100 negative examples with highest forecast probability\n",
    " - **Worst misses**: the 100 positive examples with lowest forecast probability\n",
    " - **Best correct nulls**: the 100 negative examples with lowest forecast probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5srTBokvaeh"
   },
   "source": [
    "## <font color='red'>Find extreme cases (required)</font>\n",
    "\n",
    "The next cell finds extreme cases for the pre-trained CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "4VPRgITLtJKE",
    "outputId": "41112f7c-9e62-44fc-847e-3c0fdcf9da04"
   },
   "outputs": [],
   "source": [
    "predictor_names = testing_image_dict[image_utils.PREDICTOR_NAMES_KEY]\n",
    "\n",
    "testing_predictor_matrix_denorm = (\n",
    "    testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY]\n",
    ")\n",
    "\n",
    "testing_predictor_matrix_norm, _ = image_normalization.normalize_data(\n",
    "    predictor_matrix=copy.deepcopy(testing_predictor_matrix_denorm),\n",
    "    predictor_names=predictor_names, normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "testing_classes = image_thresholding.binarize_target_images(\n",
    "    target_matrix=testing_image_dict[image_utils.TARGET_MATRIX_KEY],\n",
    "    binarization_threshold=binarization_threshold\n",
    ")\n",
    "\n",
    "testing_probs = cnn.apply_model(\n",
    "    model_object=pretrained_model_object,\n",
    "    predictor_matrix=testing_predictor_matrix_norm\n",
    ")\n",
    "\n",
    "this_dict = utils.find_extreme_examples(\n",
    "    observed_labels=testing_classes, forecast_probabilities=testing_probs,\n",
    "    num_examples_per_set=100\n",
    ")\n",
    "best_hit_indices = this_dict[utils.HIT_INDICES_KEY]\n",
    "worst_false_alarm_indices = this_dict[utils.FALSE_ALARM_INDICES_KEY]\n",
    "worst_miss_indices = this_dict[utils.MISS_INDICES_KEY]\n",
    "best_correct_null_indices = this_dict[utils.CORRECT_NULL_INDICES_KEY]\n",
    "\n",
    "extreme_example_dict_denorm = {\n",
    "    BEST_HIT_MATRIX_KEY:\n",
    "        testing_predictor_matrix_denorm[best_hit_indices, ...],\n",
    "    WORST_FALSE_ALARM_MATRIX_KEY:\n",
    "        testing_predictor_matrix_denorm[worst_false_alarm_indices, ...],\n",
    "    WORST_MISS_MATRIX_KEY:\n",
    "        testing_predictor_matrix_denorm[worst_miss_indices, ...],\n",
    "    BEST_CORRECT_NULLS_MATRIX_KEY:\n",
    "        testing_predictor_matrix_denorm[best_correct_null_indices, ...],\n",
    "    PREDICTOR_NAMES_KEY: predictor_names\n",
    "}\n",
    "\n",
    "extreme_example_dict_norm = {\n",
    "    BEST_HIT_MATRIX_KEY:\n",
    "        testing_predictor_matrix_norm[best_hit_indices, ...],\n",
    "    WORST_FALSE_ALARM_MATRIX_KEY:\n",
    "        testing_predictor_matrix_norm[worst_false_alarm_indices, ...],\n",
    "    WORST_MISS_MATRIX_KEY:\n",
    "        testing_predictor_matrix_norm[worst_miss_indices, ...],\n",
    "    BEST_CORRECT_NULLS_MATRIX_KEY:\n",
    "        testing_predictor_matrix_norm[best_correct_null_indices, ...],\n",
    "    PREDICTOR_NAMES_KEY: predictor_names\n",
    "}\n",
    "\n",
    "this_bh_matrix = utils.run_pmm_many_variables(\n",
    "    field_matrix=extreme_example_dict_denorm[BEST_HIT_MATRIX_KEY]\n",
    ")\n",
    "this_wfa_matrix = utils.run_pmm_many_variables(\n",
    "    field_matrix=extreme_example_dict_denorm[WORST_FALSE_ALARM_MATRIX_KEY]\n",
    ")\n",
    "this_wm_matrix = utils.run_pmm_many_variables(\n",
    "    field_matrix=extreme_example_dict_denorm[WORST_MISS_MATRIX_KEY]\n",
    ")\n",
    "this_bcn_matrix = utils.run_pmm_many_variables(\n",
    "    field_matrix=extreme_example_dict_denorm[BEST_CORRECT_NULLS_MATRIX_KEY]\n",
    ")\n",
    "\n",
    "extreme_example_dict_denorm_pmm = {\n",
    "    BEST_HIT_MATRIX_KEY: this_bh_matrix,\n",
    "    WORST_FALSE_ALARM_MATRIX_KEY: this_wfa_matrix,\n",
    "    WORST_MISS_MATRIX_KEY: this_wm_matrix,\n",
    "    BEST_CORRECT_NULLS_MATRIX_KEY: this_bcn_matrix,\n",
    "    PREDICTOR_NAMES_KEY: predictor_names\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcB_Ewxfzzj8"
   },
   "source": [
    "## Plot extreme cases\n",
    "\n",
    " - The next cell plots a composite (average storm) for each set of extreme cases.\n",
    " - Specifically, we plot the PMM (probability-matched means; Ebert 2001) composite.\n",
    " - PMM is similar to taking the arithmetic mean at each grid cell, but it preserves spatial structure better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xstJdgLLztBS",
    "outputId": "639c2a81-1d97-42f4-c7fa-134820de9338"
   },
   "outputs": [],
   "source": [
    "best_hits_matrix_denorm_pmm = extreme_example_dict_denorm_pmm[\n",
    "    BEST_HIT_MATRIX_KEY\n",
    "]\n",
    "worst_fa_matrix_denorm_pmm = extreme_example_dict_denorm_pmm[\n",
    "    WORST_FALSE_ALARM_MATRIX_KEY\n",
    "]\n",
    "worst_misses_matrix_denorm_pmm = extreme_example_dict_denorm_pmm[\n",
    "    WORST_MISS_MATRIX_KEY\n",
    "]\n",
    "best_nulls_matrix_denorm_pmm = extreme_example_dict_denorm_pmm[\n",
    "    BEST_CORRECT_NULLS_MATRIX_KEY\n",
    "]\n",
    "predictor_names = extreme_example_dict_denorm_pmm[PREDICTOR_NAMES_KEY]\n",
    "\n",
    "concat_predictor_matrix = numpy.stack((\n",
    "    best_hits_matrix_denorm_pmm, worst_fa_matrix_denorm_pmm,\n",
    "    worst_misses_matrix_denorm_pmm, best_nulls_matrix_denorm_pmm,\n",
    "), axis=0)\n",
    "\n",
    "temperature_matrix_kelvins = concat_predictor_matrix[\n",
    "    ..., predictor_names.index(image_utils.TEMPERATURE_NAME)\n",
    "]\n",
    "min_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 1)\n",
    "max_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 99)\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_with_barbs(\n",
    "        predictor_matrix=best_hits_matrix_denorm_pmm,\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins)\n",
    ")\n",
    "\n",
    "for i in range(axes_object_matrix.shape[0]):\n",
    "    for j in range(axes_object_matrix.shape[1]):\n",
    "        axes_object_matrix[i, j].set_title('Best hits')\n",
    "\n",
    "pyplot.show()\n",
    "print('\\n\\n')\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_with_barbs(\n",
    "        predictor_matrix=worst_fa_matrix_denorm_pmm,\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins)\n",
    ")\n",
    "\n",
    "for i in range(axes_object_matrix.shape[0]):\n",
    "    for j in range(axes_object_matrix.shape[1]):\n",
    "        axes_object_matrix[i, j].set_title('Worst false alarms')\n",
    "\n",
    "pyplot.show()\n",
    "print('\\n\\n')\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_with_barbs(\n",
    "        predictor_matrix=worst_misses_matrix_denorm_pmm,\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins)\n",
    ")\n",
    "\n",
    "for i in range(axes_object_matrix.shape[0]):\n",
    "    for j in range(axes_object_matrix.shape[1]):\n",
    "        axes_object_matrix[i, j].set_title('Worst misses')\n",
    "\n",
    "pyplot.show()\n",
    "print('\\n\\n')\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_with_barbs(\n",
    "        predictor_matrix=best_nulls_matrix_denorm_pmm,\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins)\n",
    ")\n",
    "\n",
    "for i in range(axes_object_matrix.shape[0]):\n",
    "    for j in range(axes_object_matrix.shape[1]):\n",
    "        axes_object_matrix[i, j].set_title('Best correct nulls')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ue7VO_ywrdTv"
   },
   "source": [
    "# Interpretation method 2: Saliency maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSUbOtItro5-"
   },
   "source": [
    "## Theory\n",
    "\n",
    "Most of the theory is explained in Lecture 4 (dense neural networks), but a brief recap is provided below.\n",
    "<br><br>\n",
    "\n",
    "**Saliency (Simonyan *et al.* 2014) is defined as:**\n",
    "\n",
    "<center>$s = \\frac{\\partial a}{\\partial x} \\bigg \\rvert_{x = x_0}$</center>\n",
    "\n",
    " - $a$ is the activation of a neuron in the model\n",
    " - **$x$ is one scalar predictor (in Lecture 5, one variable at one grid cell)**\n",
    " - $x_0$ is the value of $x$ in a real example\n",
    "<br><br>\n",
    "\n",
    " - Thus, saliency is a linear approximation to $\\frac{\\partial a}{\\partial x}$, linearized around the $x$-value that occurs in the example.\n",
    " - **This can be computed for all scalar predictors $x$, resulting in a map.**\n",
    "<br><br>\n",
    "\n",
    " - **Here as in Lecture 4, we will compute saliency for the output neuron.**\n",
    " - Thus, we will compute the following equation for each scalar predictor $x$, where $p$ is the probability of strong future rotation:\n",
    "<center>$s = \\frac{\\partial p}{\\partial x} \\bigg \\rvert_{x = x_0}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_gIR-48uFCg"
   },
   "source": [
    "## Saliency for random example\n",
    "\n",
    "The next cell computes and plots the saliency map for a random example in the testing data.  To interpret the plot:\n",
    "\n",
    " - Remember that saliency is $\\frac{\\partial p}{\\partial x}$, where $p$ is probability of strong future rotation and $x$ is a predictor.\n",
    " - Solid contours mean positive saliency ($p$ increases when $x$ increases).\n",
    " - Dashed contours mean positive saliency ($p$ decreases when $x$ increases).\n",
    " - Positive $u$-wind is westerly (towards the east/right).\n",
    " - Positive $v$-wind is southerly (towards the north/top)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "id": "8KP12ej50RSm",
    "outputId": "486cc569-6c48-4af9-83fe-e7eae0bab446"
   },
   "outputs": [],
   "source": [
    "predictor_matrix_denorm = testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY]\n",
    "predictor_names = testing_image_dict[image_utils.PREDICTOR_NAMES_KEY]\n",
    "\n",
    "num_examples = predictor_matrix_denorm.shape[0]\n",
    "random_index = random.randint(0, num_examples - 1)\n",
    "predictor_matrix_denorm = predictor_matrix_denorm[random_index, ...]\n",
    "\n",
    "predictor_matrix_norm, _ = image_normalization.normalize_data(\n",
    "    predictor_matrix=copy.deepcopy(predictor_matrix_denorm),\n",
    "    predictor_names=predictor_names, normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "saliency_matrix = utils.get_saliency_one_neuron(\n",
    "    model_object=pretrained_model_object,\n",
    "    predictor_matrix=numpy.expand_dims(predictor_matrix_norm, axis=0),\n",
    "    layer_name=pretrained_model_object.layers[-1].name,\n",
    "    neuron_indices=numpy.array([0], dtype=int),\n",
    "    ideal_activation=1.\n",
    ")[0, ...]\n",
    "\n",
    "saliency_matrix = saliency.smooth_saliency_maps(\n",
    "    saliency_matrices=[saliency_matrix], smoothing_radius_grid_cells=1\n",
    ")[0]\n",
    "\n",
    "temperature_matrix_kelvins = predictor_matrix_denorm[\n",
    "    ..., predictor_names.index(image_utils.TEMPERATURE_NAME)\n",
    "]\n",
    "min_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 1)\n",
    "max_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 99)\n",
    "\n",
    "wind_indices = numpy.array([\n",
    "    predictor_names.index(image_utils.U_WIND_NAME),\n",
    "    predictor_names.index(image_utils.V_WIND_NAME)\n",
    "], dtype=int)\n",
    "\n",
    "max_speed_m_s01 = numpy.percentile(\n",
    "    numpy.absolute(predictor_matrix_denorm[..., wind_indices]), 99\n",
    ")\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_sans_barbs(\n",
    "        predictor_matrix=predictor_matrix_denorm,\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins,\n",
    "        max_colour_wind_speed_m_s01=max_speed_m_s01\n",
    "    )\n",
    ")\n",
    "\n",
    "max_saliency = numpy.percentile(numpy.absolute(saliency_matrix), 99)\n",
    "\n",
    "saliency.plot_saliency_maps(\n",
    "    saliency_matrix_3d=saliency_matrix,\n",
    "    axes_object_matrix=axes_object_matrix,\n",
    "    colour_map_object=pyplot.get_cmap('Greys'),\n",
    "    max_contour_value=max_saliency,\n",
    "    contour_interval=max_saliency / 8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzsOn_-tzlf-"
   },
   "source": [
    "## Saliency for strong example\n",
    "\n",
    "The next cell computes and plots the saliency map for the strongest example in the testing data (that with the greatest max future vorticity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 783
    },
    "id": "YddM6p-Uxl6c",
    "outputId": "70d0e934-2ab8-40bc-914a-4c0ec5dfb45b"
   },
   "outputs": [],
   "source": [
    "target_matrix_s01 = testing_image_dict[image_utils.TARGET_MATRIX_KEY]\n",
    "example_index = numpy.unravel_index(\n",
    "    numpy.argmax(target_matrix_s01), target_matrix_s01.shape\n",
    ")[0]\n",
    "\n",
    "predictor_matrix_denorm = (\n",
    "    testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY][example_index, ...]\n",
    ")\n",
    "predictor_names = testing_image_dict[image_utils.PREDICTOR_NAMES_KEY]\n",
    "\n",
    "predictor_matrix_norm, _ = image_normalization.normalize_data(\n",
    "    predictor_matrix=copy.deepcopy(predictor_matrix_denorm),\n",
    "    predictor_names=predictor_names, normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "saliency_matrix = utils.get_saliency_one_neuron(\n",
    "    model_object=pretrained_model_object,\n",
    "    predictor_matrix=numpy.expand_dims(predictor_matrix_norm, axis=0),\n",
    "    layer_name=pretrained_model_object.layers[-1].name,\n",
    "    neuron_indices=numpy.array([0], dtype=int),\n",
    "    ideal_activation=1.\n",
    ")[0, ...]\n",
    "\n",
    "saliency_matrix = saliency.smooth_saliency_maps(\n",
    "    saliency_matrices=[saliency_matrix], smoothing_radius_grid_cells=1\n",
    ")[0]\n",
    "\n",
    "temperature_matrix_kelvins = predictor_matrix_denorm[\n",
    "    ..., predictor_names.index(image_utils.TEMPERATURE_NAME)\n",
    "]\n",
    "min_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 1)\n",
    "max_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 99)\n",
    "\n",
    "wind_indices = numpy.array([\n",
    "    predictor_names.index(image_utils.U_WIND_NAME),\n",
    "    predictor_names.index(image_utils.V_WIND_NAME)\n",
    "], dtype=int)\n",
    "\n",
    "max_speed_m_s01 = numpy.percentile(\n",
    "    numpy.absolute(predictor_matrix_denorm[..., wind_indices]), 99\n",
    ")\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_sans_barbs(\n",
    "        predictor_matrix=predictor_matrix_denorm,\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins,\n",
    "        max_colour_wind_speed_m_s01=max_speed_m_s01\n",
    "    )\n",
    ")\n",
    "\n",
    "max_saliency = numpy.percentile(numpy.absolute(saliency_matrix), 99)\n",
    "\n",
    "saliency.plot_saliency_maps(\n",
    "    saliency_matrix_3d=saliency_matrix,\n",
    "    axes_object_matrix=axes_object_matrix,\n",
    "    colour_map_object=pyplot.get_cmap('Greys'),\n",
    "    max_contour_value=max_saliency,\n",
    "    contour_interval=max_saliency / 8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljd9jY3L0Uc5"
   },
   "source": [
    "## Saliency for extreme cases\n",
    "\n",
    "The next two cells compute, then plot, the composite (PMM) saliency map for each set of extreme cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "W-NPmKSI0DDv",
    "outputId": "baf6f0f4-d421-4e7e-9e18-3fbb2046565a"
   },
   "outputs": [],
   "source": [
    "best_hits_saliency_matrix = utils.get_saliency_one_neuron(\n",
    "    model_object=pretrained_model_object,\n",
    "    predictor_matrix=extreme_example_dict_norm[BEST_HIT_MATRIX_KEY],\n",
    "    layer_name=pretrained_model_object.layers[-1].name,\n",
    "    neuron_indices=numpy.array([0], dtype=int),\n",
    "    ideal_activation=1.\n",
    ")\n",
    "best_hits_saliency_matrix = utils.run_pmm_many_variables(\n",
    "    field_matrix=best_hits_saliency_matrix\n",
    ")\n",
    "best_hits_saliency_matrix = saliency.smooth_saliency_maps(\n",
    "    saliency_matrices=[best_hits_saliency_matrix],\n",
    "    smoothing_radius_grid_cells=1\n",
    ")[0]\n",
    "\n",
    "worst_false_alarms_saliency_matrix = utils.get_saliency_one_neuron(\n",
    "    model_object=pretrained_model_object,\n",
    "    predictor_matrix=extreme_example_dict_norm[WORST_FALSE_ALARM_MATRIX_KEY],\n",
    "    layer_name=pretrained_model_object.layers[-1].name,\n",
    "    neuron_indices=numpy.array([0], dtype=int),\n",
    "    ideal_activation=1.\n",
    ")\n",
    "worst_false_alarms_saliency_matrix = utils.run_pmm_many_variables(\n",
    "    field_matrix=worst_false_alarms_saliency_matrix\n",
    ")\n",
    "worst_false_alarms_saliency_matrix = saliency.smooth_saliency_maps(\n",
    "    saliency_matrices=[worst_false_alarms_saliency_matrix],\n",
    "    smoothing_radius_grid_cells=1\n",
    ")[0]\n",
    "\n",
    "worst_misses_saliency_matrix = utils.get_saliency_one_neuron(\n",
    "    model_object=pretrained_model_object,\n",
    "    predictor_matrix=extreme_example_dict_norm[WORST_MISS_MATRIX_KEY],\n",
    "    layer_name=pretrained_model_object.layers[-1].name,\n",
    "    neuron_indices=numpy.array([0], dtype=int),\n",
    "    ideal_activation=1.\n",
    ")\n",
    "worst_misses_saliency_matrix = utils.run_pmm_many_variables(\n",
    "    field_matrix=worst_misses_saliency_matrix\n",
    ")\n",
    "worst_misses_saliency_matrix = saliency.smooth_saliency_maps(\n",
    "    saliency_matrices=[worst_misses_saliency_matrix],\n",
    "    smoothing_radius_grid_cells=1\n",
    ")[0]\n",
    "\n",
    "best_correct_nulls_saliency_matrix = utils.get_saliency_one_neuron(\n",
    "    model_object=pretrained_model_object,\n",
    "    predictor_matrix=extreme_example_dict_norm[BEST_CORRECT_NULLS_MATRIX_KEY],\n",
    "    layer_name=pretrained_model_object.layers[-1].name,\n",
    "    neuron_indices=numpy.array([0], dtype=int),\n",
    "    ideal_activation=1.\n",
    ")\n",
    "best_correct_nulls_saliency_matrix = utils.run_pmm_many_variables(\n",
    "    field_matrix=best_correct_nulls_saliency_matrix\n",
    ")\n",
    "best_correct_nulls_saliency_matrix = saliency.smooth_saliency_maps(\n",
    "    saliency_matrices=[best_correct_nulls_saliency_matrix],\n",
    "    smoothing_radius_grid_cells=1\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GPu8e_Ob18L7",
    "outputId": "953d4794-9e74-44a3-bc5a-38ae85d2b4f0"
   },
   "outputs": [],
   "source": [
    "predictor_names = extreme_example_dict_denorm_pmm[PREDICTOR_NAMES_KEY]\n",
    "\n",
    "concat_predictor_matrix = numpy.stack((\n",
    "    extreme_example_dict_denorm_pmm[BEST_HIT_MATRIX_KEY],\n",
    "    extreme_example_dict_denorm_pmm[WORST_FALSE_ALARM_MATRIX_KEY],\n",
    "    extreme_example_dict_denorm_pmm[WORST_MISS_MATRIX_KEY],\n",
    "    extreme_example_dict_denorm_pmm[BEST_CORRECT_NULLS_MATRIX_KEY]\n",
    "), axis=0)\n",
    "\n",
    "temperature_matrix_kelvins = concat_predictor_matrix[\n",
    "    ..., predictor_names.index(image_utils.TEMPERATURE_NAME)\n",
    "]\n",
    "min_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 1)\n",
    "max_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 99)\n",
    "\n",
    "wind_indices = numpy.array([\n",
    "    predictor_names.index(image_utils.U_WIND_NAME),\n",
    "    predictor_names.index(image_utils.V_WIND_NAME)\n",
    "], dtype=int)\n",
    "\n",
    "max_speed_m_s01 = numpy.percentile(\n",
    "    numpy.absolute(concat_predictor_matrix[..., wind_indices]), 99\n",
    ")\n",
    "\n",
    "this_max_saliency = numpy.percentile(\n",
    "    numpy.absolute(best_hits_saliency_matrix), 99\n",
    ")\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_sans_barbs(\n",
    "        predictor_matrix=extreme_example_dict_denorm_pmm[BEST_HIT_MATRIX_KEY],\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins,\n",
    "        max_colour_wind_speed_m_s01=max_speed_m_s01\n",
    "    )\n",
    ")\n",
    "\n",
    "saliency.plot_saliency_maps(\n",
    "    saliency_matrix_3d=best_hits_saliency_matrix,\n",
    "    axes_object_matrix=axes_object_matrix,\n",
    "    colour_map_object=pyplot.get_cmap('Greys'),\n",
    "    max_contour_value=this_max_saliency,\n",
    "    contour_interval=this_max_saliency / 8\n",
    ")\n",
    "\n",
    "figure_object.suptitle(\n",
    "    'Best hits (max absolute saliency = {0:.2g})'.format(this_max_saliency)\n",
    ")\n",
    "pyplot.show()\n",
    "print('\\n\\n')\n",
    "\n",
    "this_max_saliency = numpy.percentile(\n",
    "    numpy.absolute(worst_false_alarms_saliency_matrix), 99\n",
    ")\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_sans_barbs(\n",
    "        predictor_matrix=\n",
    "        extreme_example_dict_denorm_pmm[WORST_FALSE_ALARM_MATRIX_KEY],\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins,\n",
    "        max_colour_wind_speed_m_s01=max_speed_m_s01\n",
    "    )\n",
    ")\n",
    "\n",
    "saliency.plot_saliency_maps(\n",
    "    saliency_matrix_3d=worst_false_alarms_saliency_matrix,\n",
    "    axes_object_matrix=axes_object_matrix,\n",
    "    colour_map_object=pyplot.get_cmap('Greys'),\n",
    "    max_contour_value=this_max_saliency,\n",
    "    contour_interval=this_max_saliency / 8\n",
    ")\n",
    "\n",
    "figure_object.suptitle(\n",
    "    'Worst false alarms (max absolute saliency = {0:.2g})'.format(\n",
    "        this_max_saliency\n",
    "    )\n",
    ")\n",
    "pyplot.show()\n",
    "print('\\n\\n')\n",
    "\n",
    "this_max_saliency = numpy.percentile(\n",
    "    numpy.absolute(worst_misses_saliency_matrix), 99\n",
    ")\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_sans_barbs(\n",
    "        predictor_matrix=\n",
    "        extreme_example_dict_denorm_pmm[WORST_MISS_MATRIX_KEY],\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins,\n",
    "        max_colour_wind_speed_m_s01=max_speed_m_s01\n",
    "    )\n",
    ")\n",
    "\n",
    "saliency.plot_saliency_maps(\n",
    "    saliency_matrix_3d=worst_misses_saliency_matrix,\n",
    "    axes_object_matrix=axes_object_matrix,\n",
    "    colour_map_object=pyplot.get_cmap('Greys'),\n",
    "    max_contour_value=this_max_saliency,\n",
    "    contour_interval=this_max_saliency / 8\n",
    ")\n",
    "\n",
    "figure_object.suptitle(\n",
    "    'Worst misses (max absolute saliency = {0:.2g})'.format(this_max_saliency)\n",
    ")\n",
    "pyplot.show()\n",
    "print('\\n\\n')\n",
    "\n",
    "this_max_saliency = numpy.percentile(\n",
    "    numpy.absolute(best_correct_nulls_saliency_matrix), 99\n",
    ")\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_sans_barbs(\n",
    "        predictor_matrix=\n",
    "        extreme_example_dict_denorm_pmm[BEST_CORRECT_NULLS_MATRIX_KEY],\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins,\n",
    "        max_colour_wind_speed_m_s01=max_speed_m_s01\n",
    "    )\n",
    ")\n",
    "\n",
    "saliency.plot_saliency_maps(\n",
    "    saliency_matrix_3d=best_correct_nulls_saliency_matrix,\n",
    "    axes_object_matrix=axes_object_matrix,\n",
    "    colour_map_object=pyplot.get_cmap('Greys'),\n",
    "    max_contour_value=this_max_saliency,\n",
    "    contour_interval=this_max_saliency / 8\n",
    ")\n",
    "\n",
    "figure_object.suptitle(\n",
    "    'Best correct nulls (max absolute saliency = {0:.2g})'.format(\n",
    "        this_max_saliency\n",
    "    )\n",
    ")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaCVI9CB4duX"
   },
   "source": [
    "# Interpretation method 3: Class-activation maps (CAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz-y2hf24i8z"
   },
   "source": [
    "## Theory (high-level)\n",
    "\n",
    " - **Class activation is the amount of evidence for a particular class, defined at each grid point.**\n",
    " - In this notebook we consider only the positive class (strong future rotation).\n",
    " - However, we could also compute class activation for the negative class.\n",
    "<br><br>\n",
    "\n",
    "**Conceptually, are four key differences between CAMs and saliency maps:**\n",
    "\n",
    " 1. CAMs yield one value per grid point, whereas saliency maps yield one value per scalar predictor.\n",
    " 2. CAMs yield only non-negative values (in this context there is no such thing as negative evidence), whereas saliency can be positive or negative.\n",
    " 3. CAMs highlight the most important values for generating the model's actual prediction, whereas saliency maps highlight the most important values for changing the prediction.\n",
    " 4. CAMs are specific to a single convolutional layer, while saliency maps are not.\n",
    "    - CAMs produced by deeper layers are smoother, with large values confined to a smaller region.\n",
    "    - This is because deeper layers learn higher-level abstractions, which allows them to focus more selectively on important parts of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1GbhKUh7oLL"
   },
   "source": [
    "## Theory (nitty-gritty)\n",
    "\n",
    " - The original CAM method (Zhou *et al.* 2016) works only for a specific type of CNN architecture, so we use a generalized version called gradient-weighted CAM (Grad-CAM; Selvaraju *et al.* 2017).\n",
    " - The original CAM method requires the CNN to end with a global-average-pooling layer, whereas Grad-CAM allows for a wider variety of architectures, including those ending with dense layers.\n",
    "<br><br>\n",
    "\n",
    "Under Grad-CAM, class activation is defined separately for each data example, at each convolutional layer $\\mathcal{L}$, at each grid point $(i, j)$ in the feature maps output by $\\mathcal{L}$.  Specifically:\n",
    "<center>$E_{ij, k} = \\textrm{max}(\\sum\\limits_{c = 1}^{C} \\alpha_k^c A_{ij}^{c}, 0)$</center>\n",
    "<center>$\\textrm{where }\\alpha_k^c = \\frac{1}{MN} \\sum\\limits_{i = 1}^{M} \\sum\\limits_{j = 1}^{N} \\frac{\\partial \\tilde{p}_k}{\\partial A_{ij}^{c}}$</center>\n",
    "<br><br>\n",
    "\n",
    " - $A_{ij}^{c}$ is the value for channel $c$ at grid point $(i, j)$ of the feature map output by layer $\\mathcal{L}$.\n",
    " - $\\tilde{p}_k$ is the psuedo-probability (before the activation function, which here is sigmoid) of class $k$.\n",
    " - $M$ and $N$ are the number of spatial rows and columns, respectively, in the feature map output by layer $\\mathcal{L}$.\n",
    " - $C$ is the number of channels in the feature map output by layer $\\mathcal{L}$.\n",
    " - $E_{ij, k}$ is the class activation (or evidence) for class $k$ at grid point $(i, j)$.\n",
    "<br><br>\n",
    "\n",
    " - By applying the above equations to every grid point for a given convolutional layer and data example (iterating over $i$ and $j$), one can create a class-activation *map* (CAM).\n",
    " - These maps are usually overlain on the input data (predictors).\n",
    " - However, the spatial dimensions of the feature map output by layer $\\mathcal{L}$ may not match the spatial dimensions of the input, due to pooling layers in between.\n",
    " - Thus, before plotting the CAM, it is common practice to upsample the CAM to the dimensions of the input data.\n",
    " - In this notebook we use cubic interpolation to do this upsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41wH0tEg-OLI"
   },
   "source": [
    "## CAM for random example\n",
    "\n",
    "The next cell computes and plots the CAM for a random example in the testing data.  To interpret the plot:\n",
    "\n",
    " - Remember that class activation = amount of evidence for the positive class (strong future rotation).\n",
    " - Darker contours mean higher class activation.\n",
    " - Absence of contours means zero class activation.\n",
    " - The same CAM is overlain on all predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IjFsAXaP3eYd",
    "outputId": "f8b8faf1-81f5-4751-b826-3c3525b03e73"
   },
   "outputs": [],
   "source": [
    "predictor_matrix_denorm = testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY]\n",
    "predictor_names = testing_image_dict[image_utils.PREDICTOR_NAMES_KEY]\n",
    "\n",
    "num_examples = predictor_matrix_denorm.shape[0]\n",
    "random_index = random.randint(0, num_examples - 1)\n",
    "predictor_matrix_denorm = predictor_matrix_denorm[random_index, ...]\n",
    "\n",
    "predictor_matrix_norm, _ = image_normalization.normalize_data(\n",
    "    predictor_matrix=copy.deepcopy(predictor_matrix_denorm),\n",
    "    predictor_names=predictor_names, normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "temperature_matrix_kelvins = predictor_matrix_denorm[\n",
    "    ..., predictor_names.index(image_utils.TEMPERATURE_NAME)\n",
    "]\n",
    "min_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 1)\n",
    "max_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 99)\n",
    "\n",
    "wind_indices = numpy.array([\n",
    "    predictor_names.index(image_utils.U_WIND_NAME),\n",
    "    predictor_names.index(image_utils.V_WIND_NAME)\n",
    "], dtype=int)\n",
    "\n",
    "max_speed_m_s01 = numpy.percentile(\n",
    "    numpy.absolute(predictor_matrix_denorm[..., wind_indices]), 99\n",
    ")\n",
    "\n",
    "conv_layer_names = [\n",
    "    'batch_normalization_1', 'batch_normalization_3',\n",
    "    'batch_normalization_5', 'batch_normalization_7'\n",
    "]\n",
    "conv_layer_indices = numpy.array([2, 4, 6, 8], dtype=int)\n",
    "\n",
    "for i in range(len(conv_layer_names)):\n",
    "    class_activation_matrix = class_activation.run_gradcam(\n",
    "        model_object=pretrained_model_object, input_matrix=predictor_matrix_norm,\n",
    "        target_class=1, target_layer_name=conv_layer_names[i]\n",
    "    )\n",
    "\n",
    "    figure_object, axes_object_matrix = (\n",
    "        image_plotting.plot_many_predictors_sans_barbs(\n",
    "            predictor_matrix=predictor_matrix_denorm,\n",
    "            predictor_names=predictor_names,\n",
    "            min_colour_temp_kelvins=min_temp_kelvins,\n",
    "            max_colour_temp_kelvins=max_temp_kelvins,\n",
    "            max_colour_wind_speed_m_s01=max_speed_m_s01)\n",
    "    )\n",
    "\n",
    "    max_activation = numpy.percentile(class_activation_matrix, 99)\n",
    "\n",
    "    class_activation.plot_2d_cam(\n",
    "        class_activation_matrix_2d=class_activation_matrix,\n",
    "        axes_object_matrix=axes_object_matrix,\n",
    "        num_channels=predictor_matrix_norm.shape[-1],\n",
    "        colour_map_object=pyplot.get_cmap('Greys'),\n",
    "        min_contour_value=max_activation / 15,\n",
    "        max_contour_value=max_activation,\n",
    "        contour_interval=max_activation / 15\n",
    "    )\n",
    "\n",
    "    figure_object.suptitle(\n",
    "        'CAM for conv layer {0:d} of 8 (max class activation = {1:.2g})'.format(\n",
    "            conv_layer_indices[i], max_activation\n",
    "        )\n",
    "    )\n",
    "    pyplot.show()\n",
    "\n",
    "    if i != len(conv_layer_names) - 1:\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcBqhMMWB84s"
   },
   "source": [
    "## CAM for strong example\n",
    "\n",
    "The next cell computes and plots the CAM for the strongest example in the testing data (that with the greatest max future vorticity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ir9tNv8d-bro",
    "outputId": "b216d321-bb5a-4889-a7b8-5ca2ef174fda"
   },
   "outputs": [],
   "source": [
    "target_matrix_s01 = testing_image_dict[image_utils.TARGET_MATRIX_KEY]\n",
    "example_index = numpy.unravel_index(\n",
    "    numpy.argmax(target_matrix_s01), target_matrix_s01.shape\n",
    ")[0]\n",
    "\n",
    "predictor_matrix_denorm = (\n",
    "    testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY][example_index, ...]\n",
    ")\n",
    "predictor_names = testing_image_dict[image_utils.PREDICTOR_NAMES_KEY]\n",
    "\n",
    "predictor_matrix_norm, _ = image_normalization.normalize_data(\n",
    "    predictor_matrix=copy.deepcopy(predictor_matrix_denorm),\n",
    "    predictor_names=predictor_names, normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "temperature_matrix_kelvins = predictor_matrix_denorm[\n",
    "    ..., predictor_names.index(image_utils.TEMPERATURE_NAME)\n",
    "]\n",
    "min_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 1)\n",
    "max_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 99)\n",
    "\n",
    "wind_indices = numpy.array([\n",
    "    predictor_names.index(image_utils.U_WIND_NAME),\n",
    "    predictor_names.index(image_utils.V_WIND_NAME)\n",
    "], dtype=int)\n",
    "\n",
    "max_speed_m_s01 = numpy.percentile(\n",
    "    numpy.absolute(predictor_matrix_denorm[..., wind_indices]), 99\n",
    ")\n",
    "\n",
    "conv_layer_names = [\n",
    "    'batch_normalization_1', 'batch_normalization_3',\n",
    "    'batch_normalization_5', 'batch_normalization_7'\n",
    "]\n",
    "conv_layer_indices = numpy.array([2, 4, 6, 8], dtype=int)\n",
    "\n",
    "for i in range(len(conv_layer_names)):\n",
    "    class_activation_matrix = class_activation.run_gradcam(\n",
    "        model_object=pretrained_model_object, input_matrix=predictor_matrix_norm,\n",
    "        target_class=1, target_layer_name=conv_layer_names[i]\n",
    "    )\n",
    "\n",
    "    figure_object, axes_object_matrix = (\n",
    "        image_plotting.plot_many_predictors_sans_barbs(\n",
    "            predictor_matrix=predictor_matrix_denorm,\n",
    "            predictor_names=predictor_names,\n",
    "            min_colour_temp_kelvins=min_temp_kelvins,\n",
    "            max_colour_temp_kelvins=max_temp_kelvins,\n",
    "            max_colour_wind_speed_m_s01=max_speed_m_s01)\n",
    "    )\n",
    "\n",
    "    max_activation = numpy.percentile(class_activation_matrix, 99)\n",
    "\n",
    "    class_activation.plot_2d_cam(\n",
    "        class_activation_matrix_2d=class_activation_matrix,\n",
    "        axes_object_matrix=axes_object_matrix,\n",
    "        num_channels=predictor_matrix_norm.shape[-1],\n",
    "        colour_map_object=pyplot.get_cmap('Greys'),\n",
    "        min_contour_value=max_activation / 15,\n",
    "        max_contour_value=max_activation,\n",
    "        contour_interval=max_activation / 15\n",
    "    )\n",
    "\n",
    "    figure_object.suptitle(\n",
    "        'CAM for conv layer {0:d} of 8 (max class activation = {1:.2g})'.format(\n",
    "            conv_layer_indices[i], max_activation\n",
    "        )\n",
    "    )\n",
    "    pyplot.show()\n",
    "\n",
    "    if i != len(conv_layer_names) - 1:\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reMxyXkADBAJ"
   },
   "source": [
    "## CAM for extreme cases\n",
    "\n",
    "The next two cells compute, then plot, the composite (PMM) class-activation map for each set of extreme cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "P1Oo61nECzEN",
    "outputId": "da61e7ad-53e3-4e6d-fa85-205c83fb80bc"
   },
   "outputs": [],
   "source": [
    "num_examples_per_set = extreme_example_dict_norm[BEST_HIT_MATRIX_KEY].shape[0]\n",
    "\n",
    "dimensions = extreme_example_dict_norm[BEST_HIT_MATRIX_KEY].shape[:-1]\n",
    "best_hits_cam_matrix = numpy.full(dimensions, numpy.nan)\n",
    "worst_false_alarms_cam_matrix = numpy.full(dimensions, numpy.nan)\n",
    "worst_misses_cam_matrix = numpy.full(dimensions, numpy.nan)\n",
    "best_correct_nulls_cam_matrix = numpy.full(dimensions, numpy.nan)\n",
    "\n",
    "conv_layer_name = 'batch_normalization_3'\n",
    "conv_layer_index = 3\n",
    "\n",
    "for i in range(num_examples_per_set):\n",
    "    print('Have computed CAM for {0:d} of {1:d} extreme examples...'.format(\n",
    "        4 * i, 4 * num_examples_per_set\n",
    "    ))\n",
    "\n",
    "    best_hits_cam_matrix[i, ...] = class_activation.run_gradcam(\n",
    "        model_object=pretrained_model_object,\n",
    "        input_matrix=extreme_example_dict_norm[BEST_HIT_MATRIX_KEY][i, ...],\n",
    "        target_class=1, target_layer_name=conv_layer_name\n",
    "    )\n",
    "\n",
    "    worst_false_alarms_cam_matrix[i, ...] = class_activation.run_gradcam(\n",
    "        model_object=pretrained_model_object,\n",
    "        input_matrix=\n",
    "        extreme_example_dict_norm[WORST_FALSE_ALARM_MATRIX_KEY][i, ...],\n",
    "        target_class=1, target_layer_name=conv_layer_name\n",
    "    )\n",
    "\n",
    "    worst_misses_cam_matrix[i, ...] = class_activation.run_gradcam(\n",
    "        model_object=pretrained_model_object,\n",
    "        input_matrix=\n",
    "        extreme_example_dict_norm[WORST_MISS_MATRIX_KEY][i, ...],\n",
    "        target_class=1, target_layer_name=conv_layer_name\n",
    "    )\n",
    "\n",
    "    best_correct_nulls_cam_matrix[i, ...] = class_activation.run_gradcam(\n",
    "        model_object=pretrained_model_object,\n",
    "        input_matrix=\n",
    "        extreme_example_dict_norm[BEST_CORRECT_NULLS_MATRIX_KEY][i, ...],\n",
    "        target_class=1, target_layer_name=conv_layer_name\n",
    "    )\n",
    "\n",
    "print('Have computed CAM for all {0:d} extreme examples!'.format(\n",
    "    4 * num_examples_per_set\n",
    "))\n",
    "\n",
    "best_hits_cam_matrix = utils.run_pmm_one_variable(\n",
    "    field_matrix=best_hits_cam_matrix\n",
    ")\n",
    "worst_false_alarms_cam_matrix = utils.run_pmm_one_variable(\n",
    "    field_matrix=worst_false_alarms_cam_matrix\n",
    ")\n",
    "worst_misses_cam_matrix = utils.run_pmm_one_variable(\n",
    "    field_matrix=worst_misses_cam_matrix\n",
    ")\n",
    "best_correct_nulls_cam_matrix = utils.run_pmm_one_variable(\n",
    "    field_matrix=best_correct_nulls_cam_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dveAAOwAEmSX",
    "outputId": "bca7968c-bec5-4f44-d121-d546519a7667"
   },
   "outputs": [],
   "source": [
    "predictor_names = extreme_example_dict_denorm_pmm[PREDICTOR_NAMES_KEY]\n",
    "\n",
    "concat_predictor_matrix = numpy.stack((\n",
    "    extreme_example_dict_denorm_pmm[BEST_HIT_MATRIX_KEY],\n",
    "    extreme_example_dict_denorm_pmm[WORST_FALSE_ALARM_MATRIX_KEY],\n",
    "    extreme_example_dict_denorm_pmm[WORST_MISS_MATRIX_KEY],\n",
    "    extreme_example_dict_denorm_pmm[BEST_CORRECT_NULLS_MATRIX_KEY]\n",
    "), axis=0)\n",
    "\n",
    "temperature_matrix_kelvins = concat_predictor_matrix[\n",
    "    ..., predictor_names.index(image_utils.TEMPERATURE_NAME)\n",
    "]\n",
    "min_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 1)\n",
    "max_temp_kelvins = numpy.percentile(temperature_matrix_kelvins, 99)\n",
    "\n",
    "wind_indices = numpy.array([\n",
    "    predictor_names.index(image_utils.U_WIND_NAME),\n",
    "    predictor_names.index(image_utils.V_WIND_NAME)\n",
    "], dtype=int)\n",
    "\n",
    "max_speed_m_s01 = numpy.percentile(\n",
    "    numpy.absolute(concat_predictor_matrix[..., wind_indices]), 99\n",
    ")\n",
    "\n",
    "this_max_activation = numpy.percentile(best_hits_cam_matrix, 99)\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_sans_barbs(\n",
    "        predictor_matrix=extreme_example_dict_denorm_pmm[BEST_HIT_MATRIX_KEY],\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins,\n",
    "        max_colour_wind_speed_m_s01=max_speed_m_s01\n",
    "    )\n",
    ")\n",
    "\n",
    "class_activation.plot_2d_cam(\n",
    "    class_activation_matrix_2d=best_hits_cam_matrix,\n",
    "    axes_object_matrix=axes_object_matrix,\n",
    "    num_channels=len(predictor_names),\n",
    "    colour_map_object=pyplot.get_cmap('Greys'),\n",
    "    min_contour_value=this_max_activation / 15,\n",
    "    max_contour_value=this_max_activation,\n",
    "    contour_interval=this_max_activation / 15\n",
    ")\n",
    "\n",
    "figure_object.suptitle(\n",
    "    'Best hits (max class activation = {0:.2g})'.format(this_max_activation)\n",
    ")\n",
    "pyplot.show()\n",
    "print('\\n\\n')\n",
    "\n",
    "this_max_activation = numpy.percentile(worst_false_alarms_cam_matrix, 99)\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_sans_barbs(\n",
    "        predictor_matrix=\n",
    "        extreme_example_dict_denorm_pmm[WORST_FALSE_ALARM_MATRIX_KEY],\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins,\n",
    "        max_colour_wind_speed_m_s01=max_speed_m_s01\n",
    "    )\n",
    ")\n",
    "\n",
    "class_activation.plot_2d_cam(\n",
    "    class_activation_matrix_2d=worst_false_alarms_cam_matrix,\n",
    "    axes_object_matrix=axes_object_matrix,\n",
    "    num_channels=len(predictor_names),\n",
    "    colour_map_object=pyplot.get_cmap('Greys'),\n",
    "    min_contour_value=this_max_activation / 15,\n",
    "    max_contour_value=this_max_activation,\n",
    "    contour_interval=this_max_activation / 15\n",
    ")\n",
    "\n",
    "figure_object.suptitle(\n",
    "    'Worst false alarms (max class activation = {0:.2g})'.format(\n",
    "        this_max_activation\n",
    "    )\n",
    ")\n",
    "pyplot.show()\n",
    "print('\\n\\n')\n",
    "\n",
    "this_max_activation = numpy.percentile(worst_misses_cam_matrix, 99)\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_sans_barbs(\n",
    "        predictor_matrix=extreme_example_dict_denorm_pmm[WORST_MISS_MATRIX_KEY],\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins,\n",
    "        max_colour_wind_speed_m_s01=max_speed_m_s01\n",
    "    )\n",
    ")\n",
    "\n",
    "class_activation.plot_2d_cam(\n",
    "    class_activation_matrix_2d=worst_misses_cam_matrix,\n",
    "    axes_object_matrix=axes_object_matrix,\n",
    "    num_channels=len(predictor_names),\n",
    "    colour_map_object=pyplot.get_cmap('Greys'),\n",
    "    min_contour_value=this_max_activation / 15,\n",
    "    max_contour_value=this_max_activation,\n",
    "    contour_interval=this_max_activation / 15\n",
    ")\n",
    "\n",
    "figure_object.suptitle(\n",
    "    'Worst misses (max class activation = {0:.2g})'.format(this_max_activation)\n",
    ")\n",
    "pyplot.show()\n",
    "print('\\n\\n')\n",
    "\n",
    "this_max_activation = numpy.percentile(best_correct_nulls_cam_matrix, 99)\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_sans_barbs(\n",
    "        predictor_matrix=\n",
    "        extreme_example_dict_denorm_pmm[BEST_CORRECT_NULLS_MATRIX_KEY],\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins,\n",
    "        max_colour_wind_speed_m_s01=max_speed_m_s01\n",
    "    )\n",
    ")\n",
    "\n",
    "class_activation.plot_2d_cam(\n",
    "    class_activation_matrix_2d=best_correct_nulls_cam_matrix,\n",
    "    axes_object_matrix=axes_object_matrix,\n",
    "    num_channels=len(predictor_names),\n",
    "    colour_map_object=pyplot.get_cmap('Greys'),\n",
    "    min_contour_value=this_max_activation / 15,\n",
    "    max_contour_value=this_max_activation,\n",
    "    contour_interval=this_max_activation / 15\n",
    ")\n",
    "\n",
    "figure_object.suptitle(\n",
    "    'Best correct nulls (max class activation = {0:.2g})'.format(\n",
    "        this_max_activation\n",
    "    )\n",
    ")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LCXtanEF0ux"
   },
   "source": [
    "# Interpretation method 4: Backwards optimization (BWO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJQ5LQGGF4nq"
   },
   "source": [
    "## Theory (high-level)\n",
    "\n",
    " - **BWO (Erhan *et al.* 2009) creates a synthetic input that extremizes (minimizes or maximizes) the activation of a particular neuron in the model.**\n",
    " - BWO is sometimes called \"activation maximization,\" \"feature optimization,\" or \"optimal input\".\n",
    "<br><br>\n",
    "\n",
    " - **The BWO procedure is basically training in reverse.**\n",
    " - During training, gradient descent is used to adjust weights in a way that minimizes the loss function.\n",
    " - During BWO, gradient descent is used to adjust predictor values in a way that extremizes the neuron activation.\n",
    "<br><br>\n",
    "\n",
    " - **In this notebook we focus on the output neuron, whose activation is probability of strong future rotation.**\n",
    " - For example, if the goal is to maximize probability, BWO creates a prototypical strongly rotating storm (supercell).\n",
    " - If the goal is to minimize probability, BWO creates a prototypical weakly rotating storm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtNt6gWNGz1s"
   },
   "source": [
    "## Theory (nitty-gritty)\n",
    "\n",
    " - The BWO procedure involves many iterations.  At each iteration the synthetic example is updated via the rule:\n",
    "<center>$\\mathbf{X} \\leftarrow \\mathbf{X} - \\alpha \\frac{\\partial J}{\\partial \\mathbf{X}}$</center>\n",
    "<br><br>\n",
    "\n",
    " - $\\mathbf{X}$ is the tensor of predictor values.\n",
    " - $J$ is the loss function.\n",
    " - $\\frac{\\partial J}{\\partial \\mathbf{X}}$ is a gradient tensor with the same dimensions as $\\mathbf{X}$.\n",
    " - $\\alpha$ is the learning rate, usually a positive number $\\ll$ 1.\n",
    " - Both $\\alpha$ and the number of iterations are hyperparameters.\n",
    "<br><br>\n",
    "\n",
    " - In this notebook we set $J = (p - p^*)^2$, where $p$ is the CNN-generated class probability and $p^*$ is the desired probability (0.0 or 1.0).\n",
    " - Thus, the above equation can be written as:\n",
    "<center>$\\mathbf{X} \\leftarrow \\mathbf{X} - 2\\alpha (p - p^*) \\frac{\\partial p}{\\partial \\mathbf{X}}$</center>\n",
    "<br><br>\n",
    "\n",
    " - Note that $\\frac{\\partial p}{\\partial \\mathbf{X}}$ is the saliency map.\n",
    " - **Thus, BWO consists of serially adding a small fraction of the saliency map to the predictor map.**\n",
    " - Although saliency is a linear approximation, a new saliency map is created at each iteration of BWO, linearized around the new synthetic storm.\n",
    " - **Thus, BWO overcomes the linear limitation of saliency maps.**\n",
    "<br><br>\n",
    "\n",
    " - **Gradient descent** adjusts but does not initialize values, so it **requires a starting point or \"initial seed\".**\n",
    " - Some options are all-zeros, random noise, or a real data example.\n",
    " - The advantage of all-zeros and random noise is that the initial seed does not look like a real example, so the synthetic example ultimately produced is more novel (different from the initial seed).\n",
    " - The disadvantage is that, because the initial seed is unrealistic, the synthetic example is often unrealistic as well.\n",
    " - **This problem is alleviated by using a real data example, which we do in this notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEf9Jik0Is3v"
   },
   "source": [
    "## BWO for random example\n",
    "\n",
    " - The next cell runs and plots BWO for a random example in the testing data.\n",
    " - **The goal is to increase strong-rotation probability (create a supercell).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xVVe2oETIOul",
    "outputId": "1047a72e-92b5-4756-8286-46d13b85a88c"
   },
   "outputs": [],
   "source": [
    "orig_predictor_matrix_denorm = (\n",
    "    testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY]\n",
    ")\n",
    "predictor_names = testing_image_dict[image_utils.PREDICTOR_NAMES_KEY]\n",
    "\n",
    "num_examples = orig_predictor_matrix_denorm.shape[0]\n",
    "random_index = random.randint(0, num_examples - 1)\n",
    "orig_predictor_matrix_denorm = orig_predictor_matrix_denorm[random_index, ...]\n",
    "\n",
    "orig_predictor_matrix_norm, _ = image_normalization.normalize_data(\n",
    "    predictor_matrix=copy.deepcopy(orig_predictor_matrix_denorm),\n",
    "    predictor_names=predictor_names, normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "new_predictor_matrix_norm = backwards_opt.optimize_example_for_class(\n",
    "    model_object=pretrained_model_object,\n",
    "    input_matrix=numpy.expand_dims(orig_predictor_matrix_norm, axis=0),\n",
    "    target_class=1, num_iterations=1200, learning_rate=2e-4,\n",
    "    l2_weight=2e-5\n",
    ")[0][0, ...]\n",
    "\n",
    "new_predictor_matrix_denorm = image_normalization.denormalize_data(\n",
    "    predictor_matrix=new_predictor_matrix_norm,\n",
    "    predictor_names=predictor_names, normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "temperature_index = predictor_names.index(image_utils.TEMPERATURE_NAME)\n",
    "combined_temp_matrix_kelvins = numpy.concatenate((\n",
    "    orig_predictor_matrix_denorm[..., temperature_index],\n",
    "    new_predictor_matrix_denorm[..., temperature_index]\n",
    "), axis=0)\n",
    "\n",
    "min_temp_kelvins = numpy.percentile(combined_temp_matrix_kelvins, 1)\n",
    "max_temp_kelvins = numpy.percentile(combined_temp_matrix_kelvins, 99)\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_with_barbs(\n",
    "        predictor_matrix=orig_predictor_matrix_denorm,\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins\n",
    "    )\n",
    ")\n",
    "\n",
    "for i in range(axes_object_matrix.shape[0]):\n",
    "    for j in range(axes_object_matrix.shape[1]):\n",
    "        axes_object_matrix[i, j].set_title(\n",
    "            'Real example\\n(before optimization)'\n",
    "        )\n",
    "\n",
    "pyplot.show()\n",
    "print('\\n\\n')\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_with_barbs(\n",
    "        predictor_matrix=new_predictor_matrix_denorm,\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins\n",
    "    )\n",
    ")\n",
    "\n",
    "for i in range(axes_object_matrix.shape[0]):\n",
    "    for j in range(axes_object_matrix.shape[1]):\n",
    "        axes_object_matrix[i, j].set_title(\n",
    "            'Synthetic example\\n(after optimization)'\n",
    "        )\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOdK4jxuKLxe"
   },
   "source": [
    "## BWO for strong example\n",
    "\n",
    " - The next cell runs and plots BWO for the strongest example in the testing data (that with the greatest max future vorticity).\n",
    " - **The goal is to decrease strong-rotation probability (create a non-supercell).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y8VWd9uDKYSX",
    "outputId": "c5fef940-e141-4b6e-dee5-958018f2f349"
   },
   "outputs": [],
   "source": [
    "target_matrix_s01 = testing_image_dict[image_utils.TARGET_MATRIX_KEY]\n",
    "example_index = numpy.unravel_index(\n",
    "    numpy.argmax(target_matrix_s01), target_matrix_s01.shape\n",
    ")[0]\n",
    "\n",
    "orig_predictor_matrix_denorm = (\n",
    "    testing_image_dict[image_utils.PREDICTOR_MATRIX_KEY][example_index, ...]\n",
    ")\n",
    "predictor_names = testing_image_dict[image_utils.PREDICTOR_NAMES_KEY]\n",
    "\n",
    "orig_predictor_matrix_norm, _ = image_normalization.normalize_data(\n",
    "    predictor_matrix=copy.deepcopy(orig_predictor_matrix_denorm),\n",
    "    predictor_names=predictor_names, normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "new_predictor_matrix_norm = backwards_opt.optimize_example_for_class(\n",
    "    model_object=pretrained_model_object,\n",
    "    input_matrix=numpy.expand_dims(orig_predictor_matrix_norm, axis=0),\n",
    "    target_class=0, num_iterations=1200, learning_rate=1e-3,\n",
    "    l2_weight=1e-5\n",
    ")[0][0, ...]\n",
    "\n",
    "new_predictor_matrix_denorm = image_normalization.denormalize_data(\n",
    "    predictor_matrix=new_predictor_matrix_norm,\n",
    "    predictor_names=predictor_names, normalization_dict=normalization_dict\n",
    ")\n",
    "\n",
    "temperature_index = predictor_names.index(image_utils.TEMPERATURE_NAME)\n",
    "combined_temp_matrix_kelvins = numpy.concatenate((\n",
    "    orig_predictor_matrix_denorm[..., temperature_index],\n",
    "    new_predictor_matrix_denorm[..., temperature_index]\n",
    "), axis=0)\n",
    "\n",
    "min_temp_kelvins = numpy.percentile(combined_temp_matrix_kelvins, 1)\n",
    "max_temp_kelvins = numpy.percentile(combined_temp_matrix_kelvins, 99)\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_with_barbs(\n",
    "        predictor_matrix=orig_predictor_matrix_denorm,\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins\n",
    "    )\n",
    ")\n",
    "\n",
    "for i in range(axes_object_matrix.shape[0]):\n",
    "    for j in range(axes_object_matrix.shape[1]):\n",
    "        axes_object_matrix[i, j].set_title(\n",
    "            'Real example\\n(before optimization)'\n",
    "        )\n",
    "\n",
    "pyplot.show()\n",
    "print('\\n\\n')\n",
    "\n",
    "figure_object, axes_object_matrix = (\n",
    "    image_plotting.plot_many_predictors_with_barbs(\n",
    "        predictor_matrix=new_predictor_matrix_denorm,\n",
    "        predictor_names=predictor_names,\n",
    "        min_colour_temp_kelvins=min_temp_kelvins,\n",
    "        max_colour_temp_kelvins=max_temp_kelvins\n",
    "    )\n",
    ")\n",
    "\n",
    "for i in range(axes_object_matrix.shape[0]):\n",
    "    for j in range(axes_object_matrix.shape[1]):\n",
    "        axes_object_matrix[i, j].set_title(\n",
    "            'Synthetic example\\n(after optimization)'\n",
    "        )\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-hQfUnkPIpO"
   },
   "source": [
    "# References\n",
    "\n",
    "This notebook refers to a few publications, listed below.  Schwartz *et al.* (2015) documents the dataset used.\n",
    "\n",
    "Ebert, E., 2001: \"Ability of a poor mans ensemble to predict the probability and distribution of precipitation.\" *Monthly Weather Review*, **129 (10)**, 2461-2480, https://journals.ametsoc.org/doi/full/10.1175/1520-0493%282001%29129%3C2461%3AAOAPMS%3E2.0.CO%3B2.\n",
    "\n",
    "Erhan, D., Y. Bengio, A. Courville, and P. Vincent, 2009: \"Visualizing higher-layer features of a deep network.\" Technical report, University of Montr&eacute;al, https://www.researchgate.net/profile/Aaron_Courville/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network/links/53ff82b00cf24c81027da530.pdf.\n",
    "\n",
    "Schwartz, C., G. Romine, M. Weisman, R. Sobash, K. Fossell, K. Manning, and S. Trier, 2015: \"A real-time convection-allowing ensemble prediction system initialized by mesoscale ensemble Kalman filter analyses.\" *Weather and Forecasting*, **30 (5)**, 1158-1181, https://doi.org/10.1175/WAF-D-15-0013.1.\n",
    "\n",
    "Selvaraju, R., M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, 2017: \"Grad-CAM: Visual explanations from deep networks via gradient-based localization.\" *International Conference on Computer Vision*, Venice, Italy, IEEE, http://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf.\n",
    "\n",
    "Wagstaff, K., and J. Lee: \"Interpretable discovery in large image data sets.\" *arXiv e-prints*, **1806**, https://arxiv.org/abs/1806.08340.\n",
    "\n",
    "Zhou, B., A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, 2016: \"Learning deep features for discriminative localization.\" *Conference on Computer Vision and Pattern Recognition*, Las Vegas, Nevada, IEEE, https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
